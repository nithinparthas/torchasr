{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2f747c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors:  Dirk GÃ¼tlin <dirk.guetlin@gmail.com>\n",
    "#           Nicolas Barascud\n",
    "#\n",
    "# License: BSD (3-clause)\n",
    "\"\"\"\n",
    "In asrpy.asr you can find the original ASR functions (similar to MATLAB)\n",
    "as well as a high-level ASR object ready to use with MNE-Python raw data.\n",
    "\"\"\"\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "from asr_utils import (geometric_median, fit_eeg_distribution, yulewalk,\n",
    "                        yulewalk_filter, ma_filter, block_covariance)\n",
    "\n",
    "\n",
    "class ASR():\n",
    "    \"\"\"Artifact Subspace Reconstruction.\n",
    "\n",
    "    Artifact subspace reconstruction (ASR) is an automated, online,\n",
    "    component-based artifact removal method for removing transient or\n",
    "    large-amplitude artifacts in multi-channel EEG recordings [1]_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sfreq : float\n",
    "        Sampling rate of the data, in Hz.\n",
    "    cutoff: float\n",
    "        Standard deviation cutoff for rejection. X portions whose variance\n",
    "        is larger than this threshold relative to the calibration data are\n",
    "        considered missing data and will be removed. The most aggressive value\n",
    "        that can be used without losing too much EEG is 2.5. Recommended to \n",
    "        use with more conservative values ranging from 20 - 30.\n",
    "        Defaults to 20.\n",
    "    blocksize : int\n",
    "        Block size for calculating the robust data covariance and thresholds,\n",
    "        in samples; allows to reduce the memory and time requirements of the\n",
    "        robust estimators by this factor (down to Channels x Channels x Samples\n",
    "        x 16 / Blocksize bytes) (default=100).\n",
    "    win_len : float\n",
    "        Window length (s) that is used to check the data for artifact content.\n",
    "        This is ideally as long as the expected time scale of the artifacts but\n",
    "        not shorter than half a cycle of the high-pass filter that was used\n",
    "        (default=0.5).\n",
    "    win_overlap : float\n",
    "        Window overlap fraction. The fraction of two successive windows that\n",
    "        overlaps. Higher overlap ensures that fewer artifact portions are going\n",
    "        to be missed, but is slower (default=0.66).\n",
    "    max_dropout_fraction : float\n",
    "        Maximum fraction of windows that can be subject to signal dropouts\n",
    "        (e.g., sensor unplugged), used for threshold estimation (default=0.1).\n",
    "    min_clean_fraction : float\n",
    "        Minimum fraction of windows that need to be clean, used for threshold\n",
    "        estimation (default=0.25).\n",
    "    ab : 2-tuple | None\n",
    "        Coefficients (A, B) of an IIR filter that is used to shape the\n",
    "        spectrum of the signal when calculating artifact statistics. The\n",
    "        output signal does not go through this filter. This is an optional way\n",
    "        to tune the sensitivity of the algorithm to each frequency component\n",
    "        of the signal. The default filter is less sensitive at alpha and beta\n",
    "        frequencies and more sensitive at delta (blinks) and gamma (muscle)\n",
    "        frequencies. Defaults to None.\n",
    "    max_bad_chans : float\n",
    "        The maximum number or fraction of bad channels that a retained window\n",
    "        may still contain (more than this and it is removed). Reasonable range\n",
    "        is 0.05 (very clean output) to 0.3 (very lax cleaning of only coarse\n",
    "        artifacts) (default=0.2).\n",
    "    method : {'riemann', 'euclid'}\n",
    "        Method to use. If riemann, use the riemannian-modified version of\n",
    "        ASR [2]_. Currently, only euclidean ASR is supported. Defaults to\n",
    "        \"euclid\".\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    sfreq: array, shape=(n_channels, filter_order)\n",
    "        Filter initial conditions.\n",
    "    cutoff: float\n",
    "        Standard deviation cutoff for rejection.\n",
    "    blocksize : int\n",
    "        Block size for calculating the robust data covariance and thresholds.\n",
    "    win_len : float\n",
    "        Window length (s) that is used to check the data for artifact content.\n",
    "    win_overlap : float\n",
    "        Window overlap fraction.\n",
    "    max_dropout_fraction : float\n",
    "        Maximum fraction of windows that can be subject to signal dropouts.\n",
    "    min_clean_fraction : float\n",
    "        Minimum fraction of windows.\n",
    "    max_bad_chans : float\n",
    "        The maximum fraction of bad channels.\n",
    "    method : {'riemann', 'euclid'}\n",
    "        Method to use.\n",
    "    A, B: arrays\n",
    "        Coefficients of an IIR filter that is used to shape the spectrum of the\n",
    "        signal when calculating artifact statistics. The output signal does not\n",
    "        go through this filter. This is an optional way to tune the sensitivity\n",
    "        of the algorithm to each frequency component of the signal. The default\n",
    "        filter is less sensitive at alpha and beta frequencies and more\n",
    "        sensitive at delta (blinks) and gamma (muscle) frequencies.\n",
    "    M : array, shape=(channels, channels)\n",
    "        The mixing matrix to fit ASR data.\n",
    "    T : array, shape=(channels, channels)\n",
    "        The mixing matrix to fit ASR data.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kothe, C. A. E., & Jung, T. P. (2016). U.S. Patent Application No.\n",
    "       14/895,440. https://patents.google.com/patent/US20160113587A1/en\n",
    "    .. [2] Blum, S., Jacobsen, N. S. J., Bleichner, M. G., & Debener, S.\n",
    "       (2019). A Riemannian Modification of Artifact Subspace Reconstruction\n",
    "       for EEG Artifact Handling. Frontiers in Human Neuroscience, 13.\n",
    "       https://doi.org/10.3389/fnhum.2019.00141\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sfreq, cutoff=20, blocksize=100, win_len=0.5,\n",
    "                 win_overlap=0.66, max_dropout_fraction=0.1,\n",
    "                 min_clean_fraction=0.25, ab=None, max_bad_chans=0.1,\n",
    "                 method=\"euclid\"):\n",
    "\n",
    "        # set attributes\n",
    "        self.sfreq = sfreq\n",
    "        self.cutoff = cutoff\n",
    "        self.blocksize = blocksize\n",
    "        self.win_len = win_len\n",
    "        self.win_overlap = win_overlap\n",
    "        self.max_dropout_fraction = max_dropout_fraction\n",
    "        self.min_clean_fraction = min_clean_fraction\n",
    "        self.max_bad_chans = max_bad_chans\n",
    "        self.method = \"euclid\"  # NOTE: riemann is not yet available\n",
    "        self._fitted = False\n",
    "\n",
    "        # set default yule-walker filter\n",
    "        if ab is None:\n",
    "            yw_f = np.array([0, 2, 3, 13, 16, 40,\n",
    "                             np.minimum(80.0, (self.sfreq / 2.0) - 1.0),\n",
    "                             self.sfreq / 2.0]) * 2.0 / self.sfreq\n",
    "            yw_m = np.array([3, 0.75, 0.33, 0.33, 1, 1, 3, 3])\n",
    "            self.B, self.A = yulewalk(8, yw_f, yw_m)\n",
    "        else:\n",
    "            self.A, self.B = ab\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Reset state variables.\"\"\"\n",
    "        self.M = None\n",
    "        self.T = None\n",
    "\n",
    "        # TODO: The following parameters are effectively not used. Still,\n",
    "        #  they can be set manually via asr.transform(return_states=True)\n",
    "        self.R = None\n",
    "        self.carry = None\n",
    "        self.Zi = None\n",
    "        self.cov = None\n",
    "        self._fitted = False\n",
    "\n",
    "    def fit(self, raw, picks=\"eeg\", start=0, stop=None,\n",
    "            return_clean_window=False):\n",
    "        \"\"\"Calibration for the Artifact Subspace Reconstruction method.\n",
    "\n",
    "        The input to this data is a multi-channel time series of calibration\n",
    "        data. In typical uses the calibration data is clean resting EEG data\n",
    "        of data if the fraction of artifact content is below the breakdown\n",
    "        point of the robust statistics used for estimation (50% theoretical,\n",
    "        ~30% practical). If the data has a proportion of more than 30-50%\n",
    "        artifacts then bad time windows should be removed beforehand. This\n",
    "        data is used to estimate the thresholds that are used by the ASR\n",
    "        processing function to identify and remove artifact components.\n",
    "\n",
    "        The calibration data must have been recorded for the same cap design\n",
    "        from which data for cleanup will be recorded, and ideally should be\n",
    "        from the same session and same subject, but it is possible to reuse\n",
    "        the calibration data from a previous session and montage to the\n",
    "        extent that the cap is placed in the same location (where loss in\n",
    "        accuracy is more or less proportional to the mismatch in cap\n",
    "        placement).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw : instance of mne.io.Raw\n",
    "            Instance of mne.io.Raw to be used for fitting the ASR.\n",
    "            The calibration data should have been high-pass filtered (for\n",
    "            example at 0.5Hz or 1Hz using a Butterworth IIR filter), and be\n",
    "            reasonably clean not less than 30 seconds (this method is\n",
    "            typically used with 1 minute or more).\n",
    "        picks : str | list | slice | None\n",
    "            Channels used to fit the ASR. All channels should be of the same \n",
    "            type (e.g. \"eeg\", \"grads\"). Slices and lists of integers will \n",
    "            be interpreted as channel indices. In lists, channel \n",
    "            name strings (e.g., ['MEG0111', 'MEG2623'] will pick the given \n",
    "            channels. Note that channels in info['bads'] will be included if \n",
    "            their names or indices are explicitly provided. Defaults to \"eeg\".\n",
    "        start : int\n",
    "            The first sample to use for fitting the data. Defaults to 0.\n",
    "        stop : int | None\n",
    "            The last sample to use for fitting the data. If `None`, all \n",
    "            samples after `start` will be used for fitting. Defaults to None.\n",
    "        return_clean_window : Bool\n",
    "            If True, the method will return the variables `clean` (the cropped\n",
    "             dataset which was used to fit the ASR) and `sample_mask` (a\n",
    "             logical mask of which samples were included/excluded from fitting\n",
    "             the ASR). Defaults to False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        clean : array, shape=(n_channels, n_samples)\n",
    "            The cropped version of the dataset which was used to calibrate\n",
    "            the ASR. This array is a result of the `clean_windows` function\n",
    "            and no ASR was applied to it.\n",
    "        sample_mask : boolean array, shape=(1, n_samples)\n",
    "            Logical mask of the samples which were used to train the ASR.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # extract the data\n",
    "        X = raw.get_data(picks=picks, start=start, stop=stop)\n",
    "        \n",
    "        # Find artifact-free windows first\n",
    "        clean, sample_mask = clean_windows(\n",
    "            X,\n",
    "            sfreq=self.sfreq,\n",
    "            win_len=self.win_len,\n",
    "            win_overlap=self.win_overlap,\n",
    "            max_bad_chans=self.max_bad_chans,\n",
    "            min_clean_fraction=self.min_clean_fraction,\n",
    "            max_dropout_fraction=self.max_dropout_fraction)\n",
    "\n",
    "        # Perform calibration\n",
    "        self.M, self.T = asr_calibrate(\n",
    "            clean,\n",
    "            sfreq=self.sfreq,\n",
    "            cutoff=self.cutoff,\n",
    "            blocksize=self.blocksize,\n",
    "            win_len=self.win_len,\n",
    "            win_overlap=self.win_overlap,\n",
    "            max_dropout_fraction=self.max_dropout_fraction,\n",
    "            min_clean_fraction=self.min_clean_fraction,\n",
    "            ab=(self.A, self.B),\n",
    "            method=self.method)\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "        # return data if required\n",
    "        if return_clean_window:\n",
    "            return clean, sample_mask\n",
    "\n",
    "    def transform(self, raw, picks=\"eeg\", lookahead=0.25, stepsize=32, \n",
    "                  maxdims=0.66, return_states=False, mem_splits=3):\n",
    "        \"\"\"Apply Artifact Subspace Reconstruction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw : instance of mne.io.Raw\n",
    "            Instance of mne.io.Raw to be transformed by the ASR.\n",
    "        picks : str | list | slice | None\n",
    "            Channels to be transformed by the ASR. Should be the same set of \n",
    "            channels as used by `ASR.fit()`. All channels should be of the \n",
    "            same type (e.g. \"eeg\", \"grads\"). Slices and lists of integers will \n",
    "            be interpreted as channel indices. In lists, channel \n",
    "            name strings (e.g., ['MEG0111', 'MEG2623'] will pick the given \n",
    "            channels. Note that channels in info['bads'] will be included if \n",
    "            their names or indices are explicitly provided. Defaults to \"eeg\".\n",
    "        lookahead : float\n",
    "            Amount of look-ahead that the algorithm should use (in seconds). \n",
    "            This value should be between 0 (no lookahead) and WindowLength/2 \n",
    "            (optimal lookahead). The recommended value is WindowLength/2. \n",
    "            Default: 0.25\n",
    "            \n",
    "            Note: Other than in `asr_process`, the signal will be readjusted \n",
    "            to eliminate any temporal jitter and automatically readjust it to \n",
    "            the correct time points. Zero-padding will be applied to the last \n",
    "            `lookahead` portion of the data, possibly resulting in inaccuracies \n",
    "            for the final `lookahead` seconds of the recording.\n",
    "        stepsize : int\n",
    "            The steps in which the algorithm will be updated. The larger this\n",
    "            is, the faster the algorithm will be. The value must not be larger\n",
    "            than WindowLength * SamplingRate. The minimum value is 1 (update\n",
    "            for every sample) while a good value would be sfreq//3. Note that\n",
    "            an update is always performed also on the first and last sample of\n",
    "            the data chunk. Default: 32\n",
    "        max_dims : float, int\n",
    "            Maximum dimensionality of artifacts to remove. This parameter\n",
    "            denotes the maximum number of dimensions which can be removed from\n",
    "            each segment. If larger than 1, `int(max_dims)` will denote the\n",
    "            maximum number of dimensions removed from the data. If smaller\n",
    "            than 1, `max_dims` describes a fraction of total dimensions.\n",
    "            Defaults to 0.66.\n",
    "        return_states : bool\n",
    "            If True, returns a dict including the updated states {\"M\":M,\n",
    "            \"T\":T, \"R\":R, \"Zi\":Zi, \"cov\":cov, \"carry\":carry}. Defaults to\n",
    "            False.\n",
    "        mem_splits : int\n",
    "            Split the array in `mem_splits` segments to save memory.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : array, shape=(n_channels, n_samples)\n",
    "            Filtered data.\n",
    "\n",
    "        \"\"\"\n",
    "        # extract the data\n",
    "        X = raw.get_data(picks=picks)\n",
    "        \n",
    "        # add lookahead padding at the end\n",
    "        lookahead_samples = int(self.sfreq * lookahead)\n",
    "        X = np.concatenate([X,\n",
    "                            np.zeros([X.shape[0], lookahead_samples])],\n",
    "                          axis=1)\n",
    "        \n",
    "        # apply ASR\n",
    "        X = asr_process(X, self.sfreq, self.M, self.T, self.win_len,\n",
    "                        lookahead, stepsize, maxdims, (self.A, self.B),\n",
    "                        self.R, self.Zi, self.cov, self.carry,\n",
    "                        return_states, self.method, mem_splits)\n",
    "        \n",
    "        # remove lookahead portion from start\n",
    "        X = X[:, lookahead_samples:]\n",
    "        \n",
    "        # Return a modifier raw instance\n",
    "        raw = raw.copy()\n",
    "        raw.apply_function(lambda x: X, picks=picks,\n",
    "                           channel_wise=False)\n",
    "        return raw\n",
    "\n",
    "\n",
    "def asr_calibrate(X, sfreq, cutoff=20, blocksize=100, win_len=0.5,\n",
    "                  win_overlap=0.66, max_dropout_fraction=0.1,\n",
    "                  min_clean_fraction=0.25, ab=None, method='euclid'):\n",
    "    \"\"\"Calibration function for the Artifact Subspace Reconstruction method.\n",
    "    \n",
    "    This function can be used if you inted to apply ASR to a simple numpy \n",
    "    array instead of a mne.io.Raw object. It is equivalent to the MATLAB \n",
    "    implementation of asr_calibrate (except for some small differences \n",
    "    introduced by solvers for the eigenspace functions etc).\n",
    "\n",
    "    The input to this data is a multi-channel time series of calibration data.\n",
    "    In typical uses the calibration data is clean resting EEG data of ca. 1\n",
    "    minute duration (can also be longer). One can also use on-task data if the\n",
    "    fraction of artifact content is below the breakdown point of the robust\n",
    "    statistics used for estimation (50% theoretical, ~30% practical). If the\n",
    "    data has a proportion of more than 30-50% artifacts then bad time windows\n",
    "    should be removed beforehand. This data is used to estimate the thresholds\n",
    "    that are used by the ASR processing function to identify and remove\n",
    "    artifact components.\n",
    "\n",
    "    The calibration data must have been recorded for the same cap design from\n",
    "    which data for cleanup will be recorded, and ideally should be from the\n",
    "    same session and same subject, but it is possible to reuse the calibration\n",
    "    data from a previous session and montage to the extent that the cap is\n",
    "    placed in the same location (where loss in accuracy is more or less\n",
    "    proportional to the mismatch in cap placement).\n",
    "\n",
    "    The calibration data should have been high-pass filtered (for example at\n",
    "    0.5Hz or 1Hz using a Butterworth IIR filter).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape=(n_channels, n_samples)\n",
    "        *zero-mean* (e.g., high-pass filtered) and reasonably clean EEG of not\n",
    "        much less than 30 seconds (this method is typically used with 1 minute\n",
    "        or more).\n",
    "    sfreq : float\n",
    "        Sampling rate of the data, in Hz.\n",
    "    cutoff: float\n",
    "        Standard deviation cutoff for rejection. X portions whose variance\n",
    "        is larger than this threshold relative to the calibration data are\n",
    "        considered missing data and will be removed. Defaults to 20 \n",
    "        (In EEGLab's `clean_rawdata` the original threshold was set to 5, but\n",
    "        it is widely recommended to use a value higher than 20).\n",
    "    blocksize : int\n",
    "        Block size for calculating the robust data covariance and thresholds,\n",
    "        in samples; allows to reduce the memory and time requirements of the\n",
    "        robust estimators by this factor (down to n_chans x n_chans x\n",
    "        n_samples x 16 / blocksize bytes) (default=100).\n",
    "    win_len : float\n",
    "        Window length that is used to check the data for artifact content.\n",
    "        This is ideally as long as the expected time scale of the artifacts\n",
    "        but short enough to allow for several 1000 windows to compute\n",
    "        statistics over (default=0.5).\n",
    "    win_overlap : float\n",
    "        Window overlap fraction. The fraction of two successive windows that\n",
    "        overlaps. Higher overlap ensures that fewer artifact portions are\n",
    "        going to be missed, but is slower (default=0.66).\n",
    "    max_dropout_fraction : float\n",
    "        Maximum fraction of windows that can be subject to signal dropouts\n",
    "        (e.g., sensor unplugged), used for threshold estimation (default=0.1).\n",
    "    min_clean_fraction : float\n",
    "        Minimum fraction of windows that need to be clean, used for threshold\n",
    "        estimation (default=0.25).\n",
    "    ab : 2-tuple | None\n",
    "        Coefficients (A, B) of an IIR filter that is used to shape the\n",
    "        spectrum of the signal when calculating artifact statistics. The\n",
    "        output signal does not go through this filter. This is an optional way\n",
    "        to tune the sensitivity of the algorithm to each frequency component\n",
    "        of the signal. The default filter is less sensitive at alpha and beta\n",
    "        frequencies and more sensitive at delta (blinks) and gamma (muscle)\n",
    "        frequencies. Defaults to None.\n",
    "    method : {'euclid', 'riemann'}\n",
    "        Metric to compute the covariance matrix average. For now, only\n",
    "        euclidean ASR is supported.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    M : array\n",
    "        Mixing matrix.\n",
    "    T : array\n",
    "        Threshold matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    if method == \"riemann\":\n",
    "        warnings.warn(\"Riemannian ASR is not yet supported. Switching back to\"\n",
    "                      \" Euclidean ASR.\")\n",
    "        method == \"euclid\"\n",
    "\n",
    "    logging.debug('[ASR] Calibrating...')\n",
    "\n",
    "    # set number of channels and number of samples\n",
    "    [nc, ns] = X.shape\n",
    "\n",
    "    # filter the data\n",
    "    X, _zf = yulewalk_filter(X, sfreq, ab=ab)\n",
    "\n",
    "    # window length for calculating thresholds\n",
    "    N = int(np.round(win_len * sfreq))\n",
    "\n",
    "    # get block covariances\n",
    "    U = block_covariance(X, window=blocksize)\n",
    "\n",
    "    # get geometric median for each block\n",
    "    # Note: riemann mode is not yet supported, else this could be:\n",
    "    # Uavg = pyriemann.utils.mean_covariance(U, metric='riemann')\n",
    "    Uavg = geometric_median(U.reshape((-1, nc * nc)) / blocksize)\n",
    "    Uavg = Uavg.reshape((nc, nc))\n",
    "\n",
    "    # get the mixing matrix M\n",
    "    M = linalg.sqrtm(np.real(Uavg))\n",
    "\n",
    "    # sort the get the sorted eigenvecotors/eigenvalues\n",
    "    # riemann is not yet supported, else this could be PGA/nonlinear eigenvs\n",
    "    D, Vtmp = linalg.eigh(M)\n",
    "    V = Vtmp[:, np.argsort(D)]  # I think numpy sorts them automatically\n",
    "\n",
    "    # get the threshold matrix T\n",
    "    x = np.abs(np.dot(V.T, X))\n",
    "    offsets = np.int_(np.arange(0, ns - N, np.round(N * (1 - win_overlap))))\n",
    "\n",
    "    # go through all the channels and fit the EEG distribution\n",
    "    mu = np.zeros(nc)\n",
    "    sig = np.zeros(nc)\n",
    "    for ichan in reversed(range(nc)):\n",
    "        rms = x[ichan, :] ** 2\n",
    "        Y = []\n",
    "        for o in offsets:\n",
    "            Y.append(np.sqrt(np.sum(rms[o:o + N]) / N))\n",
    "        mu[ichan], sig[ichan], alpha, beta = fit_eeg_distribution(\n",
    "            Y, min_clean_fraction, max_dropout_fraction)\n",
    "    T = np.dot(np.diag(mu + cutoff * sig), V.T)\n",
    "\n",
    "    logging.debug('[ASR] Calibration done.')\n",
    "    return M, T\n",
    "\n",
    "\n",
    "def asr_process(data, sfreq, M, T, windowlen=0.5, lookahead=0.25, stepsize=32,\n",
    "                maxdims=0.66, ab=None, R=None, Zi=None, cov=None, carry=None,\n",
    "                return_states=False, method=\"euclid\", mem_splits=3):\n",
    "    \"\"\"Apply the Artifact Subspace Reconstruction method to a data array.\n",
    "\n",
    "    This function is used to clean multi-channel signal using the ASR method.\n",
    "    The required inputs are the data matrix and the sampling rate of the data.\n",
    "    \n",
    "    `asr_process` can be used if you inted to apply ASR to a simple numpy \n",
    "    array instead of a mne.io.Raw object. It is equivalent to the MATLAB \n",
    "    implementation of `asr_process` (except for some small differences \n",
    "    introduced by solvers for the eigenspace functions etc).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array, shape=(n_channels, n_samples)\n",
    "        Raw data.\n",
    "    sfreq : float\n",
    "        The sampling rate of the data.\n",
    "    M : array, shape=(n_channels, n_channels)\n",
    "        The Mixing matrix (as fitted with asr_calibrate).\n",
    "    T : array, shape=(n_channels, n_channels)\n",
    "        The Threshold matrix (as fitted with asr_calibrate).\n",
    "    windowlen : float\n",
    "        Window length that is used to check the data for artifact content.\n",
    "        This is ideally as long as the expected time scale of the artifacts\n",
    "        but short enough to allow for several 1000 windows to compute\n",
    "        statistics over (default=0.5).\n",
    "    lookahead:\n",
    "        Amount of look-ahead that the algorithm should use. Since the\n",
    "        processing is causal, the output signal will be delayed by this\n",
    "        amount. This value is in seconds and should be between 0 (no\n",
    "        lookahead) and WindowLength/2 (optimal lookahead). The recommended\n",
    "        value is WindowLength/2. Default: 0.25\n",
    "    stepsize:\n",
    "        The steps in which the algorithm will be updated. The larger this is,\n",
    "        the faster the algorithm will be. The value must not be larger than\n",
    "        WindowLength * SamplingRate. The minimum value is 1 (update for every\n",
    "        sample) while a good value would be sfreq//3. Note that an update\n",
    "        is always performed also on the first and last sample of the data\n",
    "        chunk. Default: 32\n",
    "    max_dims : float, int\n",
    "        Maximum dimensionality of artifacts to remove. This parameter\n",
    "        denotes the maximum number of dimensions which can be removed from\n",
    "        each segment. If larger than 1, `int(max_dims)` will denote the\n",
    "        maximum number of dimensions removed from the data. If smaller than 1,\n",
    "        `max_dims` describes a fraction of total dimensions. Defaults to 0.66.\n",
    "    ab : 2-tuple | None\n",
    "        Coefficients (A, B) of an IIR filter that is used to shape the\n",
    "        spectrum of the signal when calculating artifact statistics. The\n",
    "        output signal does not go through this filter. This is an optional way\n",
    "        to tune the sensitivity of the algorithm to each frequency component\n",
    "        of the signal. The default filter is less sensitive at alpha and beta\n",
    "        frequencies and more sensitive at delta (blinks) and gamma (muscle)\n",
    "        frequencies. Defaults to None.\n",
    "    R : array, shape=(n_channels, n_channels)\n",
    "        Previous reconstruction matrix. Defaults to None.\n",
    "    Zi : array\n",
    "        Previous filter conditions. Defaults to None.\n",
    "    cov : array, shape=([n_trials, ]n_channels, n_channels) | None\n",
    "        Covariance. If None (default), then it is computed from ``X_filt``.\n",
    "        If a 3D array is provided, the average covariance is computed from\n",
    "        all the elements in it. Defaults to None.\n",
    "    carry :\n",
    "        Initial portion of the data that will be added to the current data.\n",
    "        If None, data will be interpolated. Defaults to None.\n",
    "    return_states : bool\n",
    "        If True, returns a dict including the updated states {\"M\":M, \"T\":T,\n",
    "        \"R\":R, \"Zi\":Zi, \"cov\":cov, \"carry\":carry}. Defaults to False.\n",
    "    method : {'euclid', 'riemann'}\n",
    "        Metric to compute the covariance matrix average. Currently, only\n",
    "        euclidean ASR is supported.\n",
    "    mem_splits : int\n",
    "        Split the array in `mem_splits` segments to save memory.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clean : array, shape=(n_channels, n_samples)\n",
    "        Clean data.\n",
    "    state : dict\n",
    "        Output ASR parameters {\"M\":M, \"T\":T, \"R\":R, \"Zi\":Zi, \"cov\":cov,\n",
    "        \"carry\":carry}.\n",
    "\n",
    "    \"\"\"\n",
    "    if method == \"riemann\":\n",
    "        warnings.warn(\"Riemannian ASR is not yet supported. Switching back to\"\n",
    "                      \" Euclidean ASR.\")\n",
    "        method == \"euclid\"\n",
    "\n",
    "    # calculate the the actual max dims based on the fraction parameter\n",
    "    if maxdims < 1:\n",
    "        maxdims = np.round(len(data) * maxdims)\n",
    "\n",
    "    # set initial filter conditions of none was passed\n",
    "    if Zi is None:\n",
    "        _, Zi = yulewalk_filter(data, ab=ab, sfreq=sfreq,\n",
    "                                zi=np.ones([len(data), 8]))\n",
    "\n",
    "    # set the number of channels\n",
    "    C, S = data.shape\n",
    "\n",
    "    # set the number of windows\n",
    "    N = np.round(windowlen * sfreq).astype(int)\n",
    "    P = np.round(lookahead * sfreq).astype(int)\n",
    "\n",
    "    # interpolate a portion of the data if no buffer was given\n",
    "    if carry is None:\n",
    "        carry = np.tile(2 * data[:, 0],\n",
    "                        (P, 1)).T - data[:, np.mod(np.arange(P, 0, -1), S)]\n",
    "    data = np.concatenate([carry, data], axis=-1)\n",
    "\n",
    "    # splits = np.ceil(C*C*S*8*8 + C*C*8*s/stepsize + C*S*8*2 + S*8*5)...\n",
    "    splits = mem_splits  # TODO: use this for parallelization MAKE IT A PARAM FIRST\n",
    "\n",
    "    # loop over smaller segments of the data (for memory purposes)\n",
    "    last_trivial = False\n",
    "    last_R = None\n",
    "    for i in range(splits):\n",
    "\n",
    "        # set the current range\n",
    "        i_range = np.arange(i * S // splits,\n",
    "                            np.min([(i + 1) * S // splits, S]),\n",
    "                            dtype=int)\n",
    "\n",
    "        # filter the current window with yule-walker\n",
    "        X, Zi = yulewalk_filter(data[:, i_range + P], sfreq=sfreq,\n",
    "                                zi=Zi, ab=ab, axis=-1)\n",
    "\n",
    "        # compute a moving average covariance\n",
    "        Xcov, cov = \\\n",
    "            ma_filter(N,\n",
    "                      np.reshape(np.multiply(np.reshape(X, (1, C, -1)),\n",
    "                                             np.reshape(X, (C, 1, -1))),\n",
    "                                 (C * C, -1)), cov)\n",
    "\n",
    "        # set indices at which we update the signal\n",
    "        update_at = np.arange(stepsize,\n",
    "                              Xcov.shape[-1] + stepsize - 2,\n",
    "                              stepsize)\n",
    "        update_at = np.minimum(update_at, Xcov.shape[-1]) - 1\n",
    "\n",
    "        # set the previous reconstruction matrix if none was assigned\n",
    "        if last_R is None:\n",
    "            update_at = np.concatenate([[0], update_at])\n",
    "            last_R = np.eye(C)\n",
    "\n",
    "        Xcov = np.reshape(Xcov[:, update_at], (C, C, -1))\n",
    "\n",
    "        # loop through the updating intervals\n",
    "        last_n = 0\n",
    "        for j in range(len(update_at) - 1):\n",
    "\n",
    "            # get the eigenvectors/values.For method 'riemann', this should\n",
    "            # be replaced with PGA/ nonlinear eigenvalues\n",
    "            D, V = np.linalg.eigh(Xcov[:, :, j])\n",
    "\n",
    "            # determine which components to keep\n",
    "            keep = np.logical_or(D < np.sum((T @ V)**2, axis=0),\n",
    "                                 np.arange(C) + 1 < (C - maxdims))\n",
    "            trivial = np.all(keep)\n",
    "\n",
    "            # set the reconstruction matrix (ie. reconstructing artifact\n",
    "            # components using the mixing matrix)\n",
    "            if not trivial:\n",
    "                inv = pinv(np.multiply(keep[:, np.newaxis], V.T @ M))\n",
    "                R = np.real(M @ inv @ V.T)\n",
    "            else:\n",
    "                R = np.eye(C)\n",
    "\n",
    "            # apply the reconstruction\n",
    "            n = update_at[j] + 1\n",
    "            if (not trivial) or (not last_trivial):\n",
    "\n",
    "                subrange = i_range[np.arange(last_n, n)]\n",
    "\n",
    "                # generate a cosine signal\n",
    "                blend_x = np.pi * np.arange(1, n - last_n + 1) / (n - last_n)\n",
    "                blend = (1 - np.cos(blend_x)) / 2\n",
    "\n",
    "                # use cosine blending to replace data with reconstructed data\n",
    "                tmp_data = data[:, subrange]\n",
    "                data[:, subrange] = np.multiply(blend, R @ tmp_data) + \\\n",
    "                                    np.multiply(1 - blend, last_R @ tmp_data) # noqa\n",
    "\n",
    "            # set the parameters for the next iteration\n",
    "            last_n, last_R, last_trivial = n, R, trivial\n",
    "\n",
    "    # assign a new lookahead portion\n",
    "    carry = np.concatenate([carry, data[:, -P:]])\n",
    "    carry = carry[:, -P:]\n",
    "\n",
    "    if return_states:\n",
    "        return data[:, :-P], {\"M\": M, \"T\": T, \"R\": R, \"Zi\": Zi,\n",
    "                              \"cov\": cov, \"carry\": carry}\n",
    "    else:\n",
    "        return data[:, :-P]\n",
    "\n",
    "\n",
    "def clean_windows(X, sfreq, max_bad_chans=0.2, zthresholds=[-3.5, 5],\n",
    "                  win_len=.5, win_overlap=0.66, min_clean_fraction=0.25,\n",
    "                  max_dropout_fraction=0.1):\n",
    "    \"\"\"Remove periods with abnormally high-power content from continuous data.\n",
    "\n",
    "    This function cuts segments from the data which contain high-power\n",
    "    artifacts. Specifically, only windows are retained which have less than a\n",
    "    certain fraction of \"bad\" channels, where a channel is bad in a window if\n",
    "    its power is above or below a given upper/lower threshold (in standard\n",
    "    deviations from a robust estimate of the EEG power distribution in the\n",
    "    channel).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape=(n_channels, n_samples)\n",
    "        Continuous data set, assumed to be appropriately high-passed (e.g. >\n",
    "        1Hz or 0.5Hz - 2.0Hz transition band)\n",
    "    max_bad_chans : float\n",
    "        The maximum number or fraction of bad channels that a retained window\n",
    "        may still contain (more than this and it is removed). Reasonable range\n",
    "        is 0.05 (very clean output) to 0.3 (very lax cleaning of only coarse\n",
    "        artifacts) (default=0.2).\n",
    "    zthresholds : 2-tuple\n",
    "        The minimum and maximum standard deviations within which the power of\n",
    "        a channel must lie (relative to a robust estimate of the clean EEG\n",
    "        power distribution in the channel) for it to be considered \"not bad\".\n",
    "        (default=[-3.5, 5]).\n",
    "\n",
    "    The following are detail parameters that usually do not have to be tuned.\n",
    "    If you can't get the function to do what you want, you might consider\n",
    "    adapting these to your data.\n",
    "\n",
    "    win_len : float\n",
    "        Window length that is used to check the data for artifact content.\n",
    "        This is ideally as long as the expected time scale of the artifacts\n",
    "        but not shorter than half a cycle of the high-pass filter that was\n",
    "        used. Default: 1.\n",
    "    win_overlap : float\n",
    "        Window overlap fraction. The fraction of two successive windows that\n",
    "        overlaps. Higher overlap ensures that fewer artifact portions are\n",
    "        going to be missed, but is slower (default=0.66).\n",
    "    min_clean_fraction : float\n",
    "        Minimum fraction that needs to be clean. This is the minimum fraction\n",
    "        of time windows that need to contain essentially uncontaminated EEG.\n",
    "        (default=0.25)\n",
    "    max_dropout_fraction : float\n",
    "        Maximum fraction that can have dropouts. This is the maximum fraction\n",
    "        of time windows that may have arbitrarily low amplitude (e.g., due to\n",
    "        the sensors being unplugged) (default=0.1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clean : array, shape=(n_channels, n_samples)\n",
    "        Dataset with bad time periods removed.\n",
    "    sample_mask : boolean array, shape=(1, n_samples)\n",
    "        Mask of retained samples (logical array).\n",
    "\n",
    "    \"\"\"\n",
    "    assert 0 < max_bad_chans < 1, \"max_bad_chans must be a fraction !\"\n",
    "\n",
    "    # set internal variables\n",
    "    truncate_quant = [0.0220, 0.6000]\n",
    "    step_sizes = [0.01, 0.01]\n",
    "    shape_range = np.arange(1.7, 3.5, 0.15)\n",
    "    max_bad_chans = np.round(X.shape[0] * max_bad_chans)\n",
    "\n",
    "    # set data indices\n",
    "    [nc, ns] = X.shape\n",
    "    N = int(win_len * sfreq)\n",
    "    offsets = np.int_(np.round(np.arange(0, ns - N, (N * (1 - win_overlap)))))\n",
    "    logging.debug('[ASR] Determining channel-wise rejection thresholds')\n",
    "\n",
    "    wz = np.zeros((nc, len(offsets)))\n",
    "    for ichan in range(nc):\n",
    "\n",
    "        # compute root mean squared amplitude\n",
    "        x = X[ichan, :] ** 2\n",
    "        Y = np.array([np.sqrt(np.sum(x[o:o + N]) / N) for o in offsets])\n",
    "\n",
    "        # fit a distribution to the clean EEG part\n",
    "        mu, sig, alpha, beta = fit_eeg_distribution(\n",
    "            Y, min_clean_fraction, max_dropout_fraction, truncate_quant,\n",
    "            step_sizes, shape_range)\n",
    "        # calculate z scores\n",
    "        wz[ichan] = (Y - mu) / sig\n",
    "\n",
    "    # sort z scores into quantiles\n",
    "    wz[np.isnan(wz)] = np.inf  # Nan to inf\n",
    "    swz = np.sort(wz, axis=0)\n",
    "\n",
    "    # determine which windows to remove\n",
    "    if np.max(zthresholds) > 0:\n",
    "        mask1 = swz[-(np.int(max_bad_chans) + 1), :] > np.max(zthresholds)\n",
    "    if np.min(zthresholds) < 0:\n",
    "        mask2 = (swz[1 + np.int(max_bad_chans - 1), :] < np.min(zthresholds))\n",
    "\n",
    "    # combine the two thresholds\n",
    "    remove_mask = np.logical_or.reduce((mask1, mask2))\n",
    "    removed_wins = np.where(remove_mask)\n",
    "\n",
    "    # reconstruct the samples to remove\n",
    "    sample_maskidx = []\n",
    "    for i in range(len(removed_wins[0])):\n",
    "        if i == 0:\n",
    "            sample_maskidx = np.arange(\n",
    "                offsets[removed_wins[0][i]], offsets[removed_wins[0][i]] + N)\n",
    "        else:\n",
    "            sample_maskidx = np.vstack((\n",
    "                sample_maskidx,\n",
    "                np.arange(offsets[removed_wins[0][i]],\n",
    "                          offsets[removed_wins[0][i]] + N)\n",
    "            ))\n",
    "\n",
    "    # delete the bad chunks from the data\n",
    "    sample_mask2remove = np.unique(sample_maskidx)\n",
    "    if sample_mask2remove.size:\n",
    "        clean = np.delete(X, sample_mask2remove, 1)\n",
    "        sample_mask = np.ones((1, ns), dtype=bool)\n",
    "        sample_mask[0, sample_mask2remove] = False\n",
    "    else:\n",
    "        sample_mask = np.ones((1, ns), dtype=bool)\n",
    "\n",
    "    return clean, sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f8e8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
